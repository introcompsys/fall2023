---
file_format: mystnb
kernelspec:
  name: python3
---

# How does a computer represent non integer quantities?


So far we have looked at how computers represent integers in binary and how it can do math using logic gates. 

A lot of operations we do require representing quatntities that are not whole numbers.  


## Let's Try a Fixed point



Let's experiment with an 8 bit representation with 1 bit for sign and then the next 4 used for the part of the  number greater than 0 and the last 3 for the fractional part.


<!-- ![]() -->
<!-- drawing on fixed/flaot -->



so then the number:
```
01000001
```
 would be interpretted like: 



```
0-1000.001
```
positive, 8.1



in this then we can represent the numbers 0-8 for the right hand side and 0-15 on the left hand side so we get



```{code-cell} ipython3
import itertools
num_list = [str(n)+'.'+str(f) for n,f in itertools.product(range(16),range(8))]

', '.join(num_list) + ', '.join(['-'+ n for n in num_list])
```

That is it, all fo the number we can represent in this way.  


```{code-cell} ipython3
len(num_list)*2
```

```{important}
**correction**

It is still the samenumber of values, but they're spread out differenty
```


The integers would be spread out uniformly, but these are not. 

```{code-cell} python
(2**7)*2
```

## Binary fractions


What if instead, we could represent numbers with a fixed point, is to use base two fractions, just like we represent numbers in deceimal.   Instead of the points to the right of the decimal being the $\frac{1}{10}$, $\frac{1}{100}$, $\frac{1}{100}$ they are the $\frac{1}{2}$, $\frac{1}{4}$, $\frac{1}{8}$ etc.



In this fixed point we would have, for example:
`0101.1010` would be $$0*2^3 + 1*2^2 + 0*2^2 + 1*2^0 + 1*\frac{1}{2^{-1}} + 0*\frac{1}{2^{-2}} + 1*\frac{1}{2^{-}} + 0*\frac{1}{2^{-4}} = 4 + 1 + \frac{1}{2} + \frac{1}{8} = 5 + \frac{5}{8}= 5.625$$



In this case we still have a small set of numbers, but it's a different set of possible numbers.







## Review: Scientific  Notation




We can write  numbers in many different forms. We have written integers through many different bases so far.



For example in scientific contexts, we often write numbers (in base 10) like:



$$
3.45 \times 10^2 = 345
$$

or 

$$
3.45 \times 10^-2 = .0345
$$

Sometimes, we shorten these to `3.45e2` for short. 


In math, we do not have to block out how many place values we are goign to use in advance explicitly, but to build a computer we do.  So we have to decide how many bits we will use and how to use each one of them. 




## Floating point numbers are not all  exact

Before we dig into the details, let's review why this matters. 


Julia Evans asked people on Mastodon for examples how floats do weird things and then [wrote a blog post](https://jvns.ca/blog/2023/01/13/examples-of-floating-point-problems/) summarizing eight examples, and the link to the mastodon thread with even more replies. 

She also wrote a [zine](https://jvns.ca/blog/2023/06/23/new-zine--how-integers-and-floats-work/) with complementary resources.  If you would like a print copy of the zine, email Dr. Brown. 


Let's look at an example, we add `.1` together 3 times, and we get the expected result.
```{code-cell} python
.1 + .1 + .1
```




However, if we check what it's equal to, it does not equal .3
```{code-cell} python
.1 + .1 + .1 == .3
```




This is because the floating point representation is an *approximation* and there are multiple full numbers that are close to any given number that cannot be expressed exactly.
However, the display truncates since usually we want only a few significant digits.  
Even rounding does not make it work.



```{code-cell} python
round(.1,1) + round(.1,1) + round(.1,1) == round(.3,1)
```


<!-- #region -->
## Floating point IEEE standard



Now, lets see how these numbers are actually represented.



[IEEE Floating Point ](https://ieeexplore.ieee.org/document/8766229)
is what basically everyone uses, but it is technically a choice hardware manufacturers can techically make.


- Initially proposed in 1985
- Reviesd in 2008 after 7 year process that expanded
- revised in 2019 after 4 year process that mostly clarified


*IBM mainframes use their own representiton based on Hex*

Next revision is projected to 2028.

It includes specificaitons for using varying numbers of bits. In different progrmaming languages different types are represented with different standards.  

In C/C++ for example, floats are single precision floating point numbers or using 32 bits. `double` is double precision, using 64 bits. 

In python, `float` is 64 bit on a 64 bit computer. 

in Javascript, there are no integers, only floats with the precision specified by the architecture. 

### Double Standard

this is a double precision float or binary64 inthe current standard.  



it was called double in the original, offiically, so it is commonly called that.



In this case we will use: 
- 1 bit for sign
-  11 bits for exponent and 
- 52bits for the fraction part



![float bit image](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/IEEE_754_Double_Floating_Point_Format.svg/2560px-IEEE_754_Double_Floating_Point_Format.svg.png)
<!-- #endregion -->



<!-- #region -->
### How do we read a number like this?



if the sign bit is $s$ and the number represented by the exponent bits is $e$ and the 52 bits are number from right most is 0 and the last one is 52.

Then we read it as a value like: 


$$
(-1)s + \left(1 + \sum_{i=1}^{52} b_{52-i} 2^{-i}\right) \times 2^{e-1023}
$$



Note that this is $2^{-i}$ so we are working with *fractions* instead of integers in the sum.



So if we had:



```
0 01111111111 0000000000000000000000000000000000000000000000000000
```
it would represent:



$$
(-1)*0 + \left(1  + 0 \cdot 2^{-1} + \ldots  + 0 \cdot 2^{-51}+ 0 \cdot 2^{-52} \right)\times 2^{1023-1023} = 0 + (1 + 0) \times 2^0  = 1 \times 1 =  1.0
$$

In English:
- the sign bit is zero, so it is a positive number
- the exponent is 1023, so we end up multiplying by 1 
- the fraction bits represent 0, so we end up with just the 1 in the equation

[visualized](https://float.exposed/0x3ff0000000000000)

and 
```
0 01111111111 0100000000000000000000000000000000000000000000000000
```


would represent:
$$ (-1)*0 + \left(1 + 0\cdot 2^{-1} + 1\cdot 2^{-2} + \ldots  + 0\cdot 2^{-51} + 0\cdot 2^{-52} \times \right) 2^{1023-1023} = 0 + (1 + \frac{1}{2}) \times 2^0  = 1.5 \times 1 =  1.25 $$
<!-- #endregion -->

You can see this visually using [float exposed](https://float.exposed/0x3ff4000000000000)

```{important}
the URL of that link is to `float.exposed/0x3ff4000000000000`

`3ff4000000000000` is the number we want to view in hex.  Since the first 12 bits of the 64bit nubme are the sign and expontend, the first 3 characters represent those then the remaining 13 characters represent the 52 bits of the fraction.  

So `3ff=011111111` and then 4000000000000=0100000000000000000000000000000000000000000000000000`
```

````{margin}
```{hint}
To see more examples of numbers doing surprising and interesting things try [memory spy](https://memory-spy.wizardzines.com/game#)```
````

Python can give you the float representation of 

```{code-cell}  python
float.hex(1.25)
```




<!-- #region -->
### How do we get numbers into this form?




Now, let's return to our example of .1.



First we take the sign of the number for the sign bit, but then to get the exponent and fraction, we have more work to do.  

To figure this out, we'll start by manipulating the format that we need to get our number into: 


$$  \left(1 + \sum_{i=1}^{52} b_{52-i} 2^{-i}\right) \times 2^{e-1023}$$
<!-- #endregion -->



If we think about the fraction and how we add fractions, by reaching a common denominator. Since they're all powers of two, we can use the last one as our common denominator:    



$$  \left(1 + \sum_{i=1}^{52} b_{52-i} 2^{-i}\right) \times 2^{e-1023}=  \left( 1 + \frac{b_{52}}{2^{1}} + \frac{b_{51}}{2^{2}} + \cdots + \frac{b_{1}}{2^{51}} + \frac{b_{0}}{2^{52}} \right) \times 2^{e-1023} $$




Now with a common denominator:



$$ \left(\frac{2^{52}}{2^{52}} + \frac{2^{51} b_{52}}{2^{52}} + \frac{2^{50} b_{51}}{2^{52}} + \cdots + \frac{2^{1} b_{1}}{2^{52}} + \frac{2^0 b_{0}}{2^{52}}  \right) \times 2^{e-{1023}} $$


$$ \left(\frac{2^{52}+ 2^{51} b_{52}+ 2^{50} b_{51}+ \cdots + 2^{1} b_{1} + 2^0 b_{0}}{2^{52}}  \right) \times 2^{e-{1023}} $$


So then we can treat the numerator as a binary number with 53 bits (the 52 + 1)  let's call that number $J$.



$$ \frac{J}{2^{52}}  \times 2^{e-{1023}} $$



we can then combine the powers of 2 and we'll call that exponent $N$



$$ \frac{J}{2^{52-e+1023}} = \frac{ J }{2^N}$$



So i order to return to our `.1` that we want to represent, we can represent our target number as a fraction and then estimate it in the form above.



$$\frac{1}{10} \approx \frac{ J }{2^N}$$


let's isolate $J$ by multiplying both sides by $2^N$

$$J \approx \frac{2^N }{10}$$


We want to use exactly 53 bits to represent $J$ because it needs to be represented by the part of the float standard that is $1+\sum_{i=1}^{52} b_{52-i}2^{-i}$. Which is 52 bits that can be either 0 or 1, plus the left most bit is a fixed 1 from the $1+$ in the definition. 

We can use this to find what $N$ needs to be first. 

we want $J \approx  \frac{2^N }{10}$ to be greater than or equal to $2^{52}$ and less than $2^{53}$, or equivalently



 $$ 2^{52} <= \frac{2^N }{10} < 2^{53} $$



 Since $10 =8+2 =  2^3 +2^1$ then $2^3<10<2^4$
 We can also bound our quantity like this: 

 $$  \frac{2^N }{16} = \frac{2^N }{2^4} < \frac{2^N }{10} < \frac{2^N }{2^3} = \frac{2^N }{8} $$

and then we can re-write the middle comparison like this: 



 $$ 2^{N-4} <  \frac{2^N }{10} < 2^{N-3} $$


 so if we want:



 $$ 2^{52} <= \frac{2^N }{10} < 2^{53} $$



then best $N$ is 56, but we can check it.



```{code-cell} python
2**52 <= 2**56 //10 < 2**53
```



Now we can get the number we will use for $J$. So far, mathematically, we would have: 

$$J \approx \frac{2^{56}} {10}$$

but we need to find an integer value, so we divide that out and separate the `q`uotient and `r`emainder

```{code-cell} python
q,r = divmod(2**56,10)
```


Then we check the remainder to decide if we should round up by 1 or not.

```{code-cell} python
r
```


$ 6 > 5 = \frac{10}{2}$  so we round up



```{code-cell} python
J = q+1
J
```



then we chan check the length in bits of that number

Python can print it for us:
```{code-cell} python
bin(J)
```
then we note that the first 2 characters are not a part of the binary representation, but an indicator that it *is* a binary string, so we subtract 2


```{code-cell} python
len(bin(J))-2 
```

Which is 53 as expected! To get the actual number we represent in the fraction part we  want to drop the left most bit. 


Now, we go back to the $e$. We need $52-e+1023 = 56$ so we solve to find
$$e = 52+1023-56$$


```{code-cell} python
e = 52-56+1023
e
```



Python doesn't provide a binary reprsentation of floats, but it does provide a hex representation.



```python
float.hex(.1)
```



If we take the binary above, it matched this for the part before the p.
`0b1 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1010`



after the p is the exponent of $-4 = 1019-1023$. Which matches the approximation we found.  



We can also check like this: 

```python
(J)/2**56 == .1
```



this confirms that the approximation we found is the same as the float representation of .1.



## Prepare for Next Class

```{include} ../_prepare/2023-11-28.md
```

## Review today's class

```{include} ../_review/2023-11-21.md
```



## More Practice

```{include} ../_practice/2023-11-21.md
```



## Experience Report Evidence

## Questions After Today's Class 


### Why can't systems just interpret decimal numbers like humans can?


Because it has to be represented mechanically or electrically for it to be a machine. 


### My question is how bad can the approximations get when calculating floats? How much precision gets lost when dealing with fractional numbers?

The quality of the approximation varies depending on the particular number. 


At the bottom of 
[float exposed](https://float.exposed/0x3ff0000000000000) you can see how close the next value is. 

For 1 (the link above) the next number is a little more than 2.22e-16 away. 

For [1532.625](https://float.exposed/0x44bf9400) the next value is  1.22e-4 away. 

For [4503599627370497.0](https://float.exposed/0x4330000000000001)  to [9007199254740991.0](https://float.exposed/0x433fffffffffffff)the next value is 1.0 away, that is we can only represent integers between those two values, nothing in between. 


### Did the people that came from an academia background typically design and implement these low-level systems or people in industry?

A lot of this work was deep collaborations between academia and industry.  Some of it occured in universities, some of it was at places like  Bell Laboratory. That was a research instittue inside of the for profit Bell Telephone Company. 


### How can I practice what we learned today for better understanding? 

This is a hard topic and it will come back up in CSC411.  

Also, I recommend reading these notes carefully, and trying the float.exposed site. 

### Does this process of going through all the bits increase in time with more bytes? Or is there a set number of bits?

This standard is a fixed size of 64 bits, but ther are others with different numbers of bits. 


### How different was the initial standard from the one that exists today?

The initial standard defined fewer basic formats, more were introduced in 2008.  Official IEEE standards are not free so they're slightly harder to read easily.  

### How are we able to assign 0.1 to a variable despite that?


We do not actually get a .1 for doing math.  It is close, but not exact.  Unless your programming environment has a different base type. 